{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "msdoc": "how-to-track-experiments.md",
    "name": "load_ws",
    "tags": [
     "install"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are currently using version 1.5.0 of the Azure ML SDK\n",
      "\n",
      "Workspace name: team06\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core import Experiment, Workspace\n",
    "from azureml.core import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")\n",
    "print(\"\")\n",
    "\n",
    "# Log In to Azure ML Workspace\n",
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"76f90eb1-fb9a-4446-9875-4d323d6455ad\")\n",
    "\n",
    "ws = Workspace.from_config(auth=interactive_auth)\n",
    "print('Workspace name: ' + ws.name, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset from Azure ML Workspace\n",
    "aml_dataset = Dataset.get_by_name(ws, 'train_ds', version='latest')\n",
    "\n",
    "df = aml_dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, train_cluster) :\n",
    "    df['gender'] = df['gender'].map({'лќ':0, 'лю': 1})\n",
    "    if train_cluster:\n",
    "        df['group_cat'] = df['group'].map({'test':1, 'control':0})\n",
    "        df['target'] = 0\n",
    "        df.loc[(df['group_cat'] == 1) & (df['response_att'] == 1), 'target'] = 1\n",
    "        df.loc[(df['group_cat'] == 0) & (df['response_att'] == 0), 'target'] = 1\n",
    "\n",
    "    df['uplift'] = 0\n",
    "\n",
    "\n",
    "    for name_col in ['cheque_count_12m', 'cheque_count_3m', 'cheque_count_6m', \n",
    "                     'k_var_disc_share_15d', 'k_var_disc_share_1m', 'k_var_disc_share_3m', 'k_var_disc_share_6m',\n",
    "                     'k_var_sku_price_15d', 'k_var_sku_price_1m', 'k_var_sku_price_3m', 'k_var_sku_price_6m',\n",
    "                     'k_var_count_per_cheque_15d', 'k_var_count_per_cheque_1m',\n",
    "                     'k_var_count_per_cheque_3m', 'k_var_count_per_cheque_6m',\n",
    "                    'sale_count_12m', 'sale_count_3m', 'sale_count_6m', 'sale_sum_12m', 'sale_sum_3m', 'sale_sum_6m']:\n",
    "        need_col = [col for col in df.columns if col.startswith(name_col)]\n",
    "        df['sum_' + name_col] = df.loc[:, need_col].sum(axis = 1)\n",
    "        df['max_' + name_col] = df.loc[:, need_col].max(axis = 1)\n",
    "\n",
    "    kmeans_columns = ['response_viber', 'k_var_days_between_visits_15d', 'max_cheque_count_6m', 'response_sms', \n",
    "                       'perdelta_days_between_visits_15_30d', 'max_cheque_count_12m', 'k_var_days_between_visits_1m', \n",
    "                       'stdev_days_between_visits_15d', 'sum_k_var_sku_price_6m', 'k_var_days_between_visits_3m']\n",
    "\n",
    "    if train_cluster:\n",
    "        km = KMeans(n_clusters = 5, random_state = 1)\n",
    "        max_kmeans_columns = df[kmeans_columns].abs().max().values\n",
    "        mean_kmeans_columns = df[kmeans_columns].mean()\n",
    "        df['kmeans'] = km.fit_predict( df[kmeans_columns].fillna(mean_kmeans_columns) /  max_kmeans_columns)\n",
    "        \n",
    "        groupby_dict_mean = {}\n",
    "        for col in kmeans_columns: \n",
    "            groupby_dict_mean[col] = df.groupby(['kmeans'])[col].mean()\n",
    "            df['gp_diff_' + col] = df[col] - df['kmeans'].map( groupby_dict_mean[col]  )\n",
    "            \n",
    "        with open('kmeans.pickle', 'wb') as f:\n",
    "            pickle.dump([max_kmeans_columns, mean_kmeans_columns, km, groupby_dict_mean], f)\n",
    "            \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        with open('kmeans.pickle', 'rb') as f:\n",
    "            max_kmeans_columns, mean_kmeans_columns, km, groupby_dict_mean = pickle.load(f)\n",
    "        df['kmeans'] = km.predict( df[kmeans_columns].fillna(mean_kmeans_columns) /  max_kmeans_columns)\n",
    "\n",
    "\n",
    "        for col in kmeans_columns: \n",
    "            df['gp_diff_' + col] = df[col] - df['kmeans'].map( groupby_dict_mean[col] )\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataset(df, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_dataset_test = Dataset.get_by_name(ws, 'test_ds', version='latest')\n",
    "df_test = aml_dataset_test.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = create_dataset(df_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "msdoc": "how-to-track-experiments.md",
    "name": "load_data"
   },
   "outputs": [],
   "source": [
    "drop_cols = ['CardHolder', 'target', 'group', 'response_att', 'predict', 'group_cat', 'uplift']\n",
    "train_cols = [col for col in df.columns if col not in drop_cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df['target'], stratify = df['group_cat'],\n",
    "                                                    test_size=200000, random_state=322,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_metric(answers, take_top_ratio=0.25):\n",
    "    answers = answers.copy()\n",
    "    answers.sort_values(by='uplift', inplace=True, ascending=False)\n",
    "    n_samples = int(np.ceil(answers.shape[0] * take_top_ratio))\n",
    "    answers = answers.iloc[:n_samples, :]\n",
    "    answers_test = answers[answers['group'] == 'test']['response_att'].sum() / \\\n",
    "                   answers[answers['group'] == 'test'].shape[0]\n",
    "    answers_control = answers[answers['group'] == 'control']['response_att'].sum() / \\\n",
    "                      answers[answers['group'] == 'control'].shape[0]\n",
    "    return (answers_test - answers_control) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WinModel():\n",
    "    \n",
    "    def __init__(self, params, n_seeds, n_folds, train_cols):\n",
    "        self.params = params\n",
    "        self.n_seeds = n_seeds\n",
    "        self.n_folds = n_folds\n",
    "        self.train_cols = train_cols\n",
    "        self.bst_list = None\n",
    "    \n",
    "    def fit_lgb(self, X, y):\n",
    "        target = 'target'\n",
    "        bst_list = []\n",
    "        self.val_score_list = []\n",
    "        for i in tqdm(range(self.n_seeds)):\n",
    "            self.params['random_state'] = i\n",
    "\n",
    "            tr = lgb.Dataset(np.array(X[self.train_cols]), np.array(y))\n",
    "\n",
    "            bst = lgb.train(self.params, tr, num_boost_round = 120)\n",
    "\n",
    "            bst_list += [bst]\n",
    "                \n",
    "        self.bst_list = bst_list\n",
    "        \n",
    "    \n",
    "    def predict(self, df):\n",
    "        pred = [bst.predict(np.array(df[self.train_cols])) for bst in self.bst_list]\n",
    "        pred = np.sum(pred, axis = 0)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = 'outputs/model.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = ['kmeans', 'gp_diff_max_cheque_count_6m', 'response_sms', 'gp_diff_max_cheque_count_12m', \n",
    "              'k_var_days_between_visits_1m', 'max_cheque_count_6m', 'k_var_days_between_visits_15d', \n",
    "              'perdelta_days_between_visits_15_30d', 'months_from_register', 'gp_diff_k_var_days_between_visits_3m', \n",
    "              'gp_diff_stdev_days_between_visits_15d', 'gp_diff_response_sms', 'gp_diff_k_var_days_between_visits_15d', \n",
    "              'max_cheque_count_12m', 'response_viber', 'gp_diff_k_var_days_between_visits_1m', \n",
    "              'gp_diff_perdelta_days_between_visits_15_30d', 'k_var_cheque_15d', 'stdev_days_between_visits_15d', \n",
    "              'gp_diff_response_viber', 'k_var_cheque_3m', 'k_var_days_between_visits_3m', 'k_var_cheque_category_width_15d', \n",
    "              'k_var_disc_per_cheque_15d', 'max_k_var_count_per_cheque_1m', 'food_share_15d', 'promo_share_15d', \n",
    "              'stdev_discount_depth_15d', 'max_k_var_sku_price_3m', 'age', 'max_cheque_count_3m', 'food_share_1m', \n",
    "              'max_k_var_disc_share_3m', 'k_var_sku_per_cheque_15d', 'sum_cheque_count_6m', 'gp_diff_sum_k_var_sku_price_6m', \n",
    "              'mean_discount_depth_15d', 'sum_k_var_disc_share_15d', 'max_k_var_count_per_cheque_3m', \n",
    "              'sum_k_var_disc_share_6m', 'max_k_var_disc_share_15d', 'max_k_var_sku_price_1m', 'cheque_count_12m_g48', \n",
    "              'k_var_cheque_group_width_15d', 'disc_sum_6m_g34', 'sale_sum_3m_g33', 'sum_k_var_sku_price_6m', \n",
    "              'k_var_sku_price_6m_g49', 'sum_sale_count_12m', 'main_format', 'sum_sale_sum_12m', 'sum_k_var_sku_price_3m',\n",
    "              'k_var_discount_depth_15d', 'k_var_sku_price_6m_g27', 'k_var_sku_price_6m_g24', 'sale_sum_3m_g26', \n",
    "              'cheque_count_3m_g25', 'k_var_disc_share_6m_g54', 'cheque_count_3m_g20', 'k_var_disc_share_3m_g24', \n",
    "              'k_var_disc_share_15d_g24', 'max_k_var_disc_share_6m', 'max_k_var_disc_share_1m', 'k_var_sku_price_6m_g48',\n",
    "              'sum_cheque_count_12m', 'cheque_count_6m_g41', 'k_var_sku_price_3m_g48', 'sale_sum_12m_g27', \n",
    "              'k_var_count_per_cheque_1m_g49', 'k_var_disc_share_6m_g27']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace=ws, name=\"fs70full_df-experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [{'learning_rate':0.1,'max_depth':5, 'objective':'binary',\n",
    "            'min_data_in_leaf': 2500},\n",
    "              {'learning_rate':0.1,'max_depth':5, 'objective':'binary',\n",
    "            'min_data_in_leaf': 1500,},\n",
    "              {'learning_rate':0.1,'max_depth':6, 'objective':'binary',\n",
    "            'min_data_in_leaf': 2500, },\n",
    "              {'learning_rate':0.1,'max_depth':6, 'objective':'binary',\n",
    "            'min_data_in_leaf': 1500,}]\n",
    "\n",
    "\n",
    "run =  experiment.start_logging()\n",
    "model_list = []\n",
    "for params in params_list:\n",
    "        \n",
    "        model = WinModel(params, 10, 2, train_cols)\n",
    "        model.fit_lgb(df, df['target'])\n",
    "        model_list += model.bst_list\n",
    "joblib.dump(value = model.bst_list, filename = model_file_name)\n",
    "run.upload_file(name = model_file_name, path_or_stream = model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['uplift'] = 0\n",
    "\n",
    "for model in model_list:\n",
    "    df_test['uplift'] += model.predict(df_test[train_cols], num_iteration = 105)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['CardHolder', 'uplift']].to_csv(\"final.csv\", index = None, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "query history"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "model = best_run.register_model(model_name='best_model', model_path='outputs/model.pkl')"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "category": "tutorial",
  "compute": [
   "Local"
  ],
  "datasets": [
   "Diabetes"
  ],
  "deployment": [
   "Azure Container Instance"
  ],
  "exclude_from_index": false,
  "framework": [
   "None"
  ],
  "friendly_name": "Train and deploy a model using Python SDK",
  "index_order": 1,
  "kernelspec": {
   "display_name": "azureml_py36_automl",
   "language": "python",
   "name": "conda-env-azureml_py36_automl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "tags": [
   "None"
  ],
  "task": "Training and deploying a model from a notebook"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
